{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Bias Invariant RNA-Seq Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Welcome to this notebook where we'll run an example using our novel RNA-Seq annotation method.\n",
    "\n",
    "In this notebook you will be able to reproduce some of our results yourself!\n",
    "\n",
    "We'll go through the following steps:\n",
    "\n",
    "<ol>\n",
    "<li>Install the source code for the DA model as published on our github repo</li>\n",
    "<li>Load training, test and bias injection data sets</li>\n",
    "<li>Run and evaluate a full training cycle for our model</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Befor we can import we first have to install the package, run the following cell.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /projectbig/jupyternotebook/rnaseq_augmentation/rna_augment\n",
      "Building wheels for collected packages: rna-augment\n",
      "  Building wheel for rna-augment (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rna-augment: filename=rna_augment-1.0-cp37-none-any.whl size=4821 sha256=5a330fd1a116c87151c1d91740a555c9d846f07f3e41b57b700b186f4ec3fe2b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-n0kp13i6/wheels/6b/93/40/f3e9867b3e873b21b81882c92ddac3850b395e576c949465e7\n",
      "Successfully built rna-augment\n",
      "Installing collected packages: rna-augment\n",
      "Successfully installed rna-augment-1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from src import da_model, load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Next we'll load some source, target and bias data. We'll reproduce the results for the DA G+S-T expereiment for tissue prediction as described in our paper. <br>\n",
    "As source data we load all the GTEx data originally used in the paper as well as all the SRA data as bias data. As target we'll load a random subset (frac=0.5) of the origianl TCGA test data. Using a subset of the TCGA data saved some space and time but will lead to comparable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "source, target, bias = load_data.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "BIRA comes with a number of hyperparameters that can be chosen freedly, here we provide the parameters chosen in the paper for this experiment.\n",
    "\n",
    "<ul>\n",
    "    <li>source_layers: a list of integers representing the number of nodes to be used per layer for the source and bias mapper, [512] will create one layer with 512 nodes</li>\n",
    "    <li>classifier_layers: a list of integers representing the number of nodes to be used per layer for the classifier layer, [] will only create a single output layer with n=classes</li>\n",
    "    <li>lr: learning rate applied in the second training cycle\n",
    "    <li>classes: number of classes in the data</li>\n",
    "    <li>batch_size: batch size</li>\n",
    "    <li>margin: size of margin applied in triplet loss\n",
    "    <li>print: True / False, if test accuracy should be printed after every epoch during the second training cycle    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "config = {'mapper_layers': [512],\n",
    "          'classifier_layers': [],\n",
    "          'lr': 0.0005,\n",
    "          'classes': 16,\n",
    "          'batch_size': 64,\n",
    "          'margin': 5,\n",
    "          'print': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Finally we start with the first training cyle, here we train the source mapper and the classification layer as a vanilla MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6729802860456127"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = da_model.DaModel(source, target, bias, config=config)\n",
    "model.train_source_mapper(epochs=10)\n",
    "model.eval_source_mapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The accuracy above is what we achieved using GTEx to train a MLP to predict TCGA.\n",
    "Note that we use a different network than the MLP G-T we present in the paper, so results my vary.\n",
    "Let's see if we can do better by injecting some SRA data set biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6590645535369154\n",
      "0.8051797448782373\n",
      "0.845380749903363\n",
      "0.8442211055276382\n",
      "0.8554310011596443\n",
      "0.8596830305373019\n",
      "0.8763045999226904\n",
      "0.8778507924236567\n",
      "0.872825666795516\n",
      "0.8755315036722072\n"
     ]
    }
   ],
   "source": [
    "model.train_bias_mapper(epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
