{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Bias Invariant RNA-Seq Annotation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Welcome to this notebook where we'll run an example using our novel RNA-Seq annotation method.\n",
    "\n",
    "In this notebook you will be able to reproduce some of our results yourself!\n",
    "\n",
    "We'll go through the following steps:\n",
    "\n",
    "<ol>\n",
    "<li>Install the source code for the DA model as published on our github repo</li>\n",
    "<li>Load training, test and bias injection data sets</li>\n",
    "<li>Run and evaluate a full training cycle for our model</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Befor we can import we first have to install the package, run the following cell.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from src import da_model, load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Next we'll load some source, target and bias data. We'll reproduce the results for the DA G+S-T expereiment for tissue prediction as described in our paper. <br>\n",
    "As source data we load all the GTEx data originally used in the paper as well as all the SRA data as bias data. As target we'll load a random subset (frac=0.5) of the origianl TCGA test data. Using a subset of the TCGA data saved some space and time but will lead to comparable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "source, target, bias = load_data.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "BIRA comes with a number of hyperparameters that can be chosen freedly, here we provide the parameters chosen in the paper for this experiment.\n",
    "\n",
    "<ul>\n",
    "    <li>source_layers: a list of integers representing the number of nodes to be used per layer for the source and bias mapper, [512] will create one layer with 512 nodes</li>\n",
    "    <li>classifier_layers: a list of integers representing the number of nodes to be used per layer for the classifier layer, [] will only create a single output layer with n=classes</li>\n",
    "    <li>lr: learning rate applied in the second training cycle\n",
    "    <li>classes: number of classes in the data</li>\n",
    "    <li>batch_size: batch size</li>\n",
    "    <li>margin: size of margin applied in triplet loss\n",
    "    <li>print: True / False, if test accuracy should be printed after every epoch during the second training cycle    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "config = {'mapper_layers': [512],\n",
    "          'classifier_layers': [],\n",
    "          'lr': 0.0005,\n",
    "          'classes': 16,\n",
    "          'batch_size': 64,\n",
    "          'margin': 5,\n",
    "          'print': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Finally we start with the first training cyle, here we train the source mapper and the classification layer as a vanilla MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = da_model.DaModel(source, target, bias, config=config)\n",
    "model.train_source_mapper(epochs=10)\n",
    "model.eval_source_mapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The accuracy above is what we achieved using GTEx to train a MLP to predict TCGA.\n",
    "Note that we use a different network than the MLP G-T we present in the paper, so results my vary.\n",
    "Let's see if we can do better by injecting some SRA data set biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.train_bias_mapper(epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
