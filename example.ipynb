{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIRA: Bias Invariant RNA-Seq Annotation Using Domain Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this notebook where we'll run an example using our novel RNA-Seq annotation method.\n",
    "\n",
    "In this notebook you will be able to reproduce some of our results yourself!\n",
    "\n",
    "We'll go through the following steps:\n",
    "\n",
    "<ol>\n",
    "<li>Install the source code for BIRA as published on our github repo</li>\n",
    "<li>Load training, test and bias injection data sets</li>\n",
    "<li>Run and evaluate a full training cycle for BIRA</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Befor we can import BIRA we first have to install the package, run the following cell.<br>\n",
    "After installing you need to <b> restart the kernel </b>. To do so, select Kernal -> Restart Kernal..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /projectbig/jupyternotebook/rnaseq_augmentation/rna_augment\n",
      "Building wheels for collected packages: rna-augment\n",
      "  Building wheel for rna-augment (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rna-augment: filename=rna_augment-1.0-cp37-none-any.whl size=4605 sha256=ae28f26622ca23407dd3ac1d446c475512a1587841ee093435b4256fbb3c3f36\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-dr817v6z/wheels/6b/93/40/f3e9867b3e873b21b81882c92ddac3850b395e576c949465e7\n",
      "Successfully built rna-augment\n",
      "Installing collected packages: rna-augment\n",
      "  Found existing installation: rna-augment 1.0\n",
      "    Uninstalling rna-augment-1.0:\n",
      "      Successfully uninstalled rna-augment-1.0\n",
      "Successfully installed rna-augment-1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import bira, load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll load some training data, target data and bias data. We'll reproduce the results for the BIRA G+S-T expereiment as described in our paper. <br>\n",
    "For \"source\" we load all the GTEx data originally used in the paper as well as all the SRA data as \"bias\". For \"target\" we'll load a random subset (frac=0.5) of the origianl TCGA test data. Using a subset of the TCGA data saved some space but will lead to comparable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source, target, bias = load_data.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIRA comes with a number of hyperparameters that can be chosen freedly, here we provide the parameters chosen in the paper for this experiment.\n",
    "\n",
    "<ul>\n",
    "    <li>source_layers: a list of integers representing the number of nodes to be used per layer for the source and bias mapper, [512] will create one layer with 512 nodes</li>\n",
    "    <li>classifier_layers: a list of integers representing the number of nodes to be used per layer for the classifier layer, [] will only create a single output layer with n=classes</li>\n",
    "    <li>lr: learning rate applied in the second training cycle\n",
    "    <li>classes: number of classes in the data</li>\n",
    "    <li>batch_size: batch size</li>\n",
    "    <li>margin: size of margin applied in triplet loss\n",
    "    <li>print: True / False, if test accuracy should be printed after every epoch during the second training cycle    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'source_layers': [512],\n",
    "      'classifier_layers': [],\n",
    "      'lr': 0.0005,\n",
    "      'classes': 16,\n",
    "      'batch_size': 64,\n",
    "      'margin': 11,\n",
    "         'print': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we start with the first training cyle, here we train the source mapper and the classification layer as a vanilla MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.622342481638964\n"
     ]
    }
   ],
   "source": [
    "model = bira.Bira(source, target, bias, config=config)\n",
    "model.train_source_mapper(epochs=10)\n",
    "model.eval_source_mapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy above is what we achieved using GTEx to train a MLP to predict TCGA, let's see if we can do better by injecting some SRA data set biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6899884035562428\n",
      "0.8001546192500967\n",
      "0.8229609586393506\n",
      "0.8438345574023965\n",
      "0.8573637417858523\n",
      "0.860456126787785\n",
      "0.8689601855431001\n",
      "0.8685736374178585\n",
      "0.8689601855431001\n",
      "0.8797835330498647\n"
     ]
    }
   ],
   "source": [
    "model.train_da(epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
